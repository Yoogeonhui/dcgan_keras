{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Activation, BatchNormalization, Reshape, UpSampling2D, Conv2D, MaxPooling2D, \\\n",
    "    Conv2DTranspose, LeakyReLU, Flatten\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD, Adam\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "import cv2\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.losses import categorical_crossentropy\n",
    "\n",
    "what_now = 0\n",
    "imageDir = './image'\n",
    "directoryList = listdir(imageDir)\n",
    "size_of_data = len(directoryList)\n",
    "size = 64\n",
    "\n",
    "\n",
    "def nextBatch(batch_size = 100):\n",
    "    global what_now\n",
    "    save = np.zeros(shape = (batch_size, size, size, 3), dtype=np.float32)\n",
    "    loadList = []\n",
    "    isthisEnd = False\n",
    "    if what_now+batch_size > size_of_data:\n",
    "        loadList = directoryList[what_now:size_of_data]\n",
    "        loadList.extend(directoryList[0:batch_size-(size_of_data-what_now)])\n",
    "        what_now = batch_size-(size_of_data-what_now)\n",
    "        isthisEnd = True\n",
    "    else:\n",
    "        loadList = directoryList[what_now:what_now+batch_size]\n",
    "        what_now += batch_size\n",
    "    for i, name in enumerate(loadList):\n",
    "        image = cv2.imread(imageDir+'/'+name, cv2.IMREAD_COLOR)\n",
    "        resized = cv2.resize(image, (size,size))\n",
    "        save[i] = resized\n",
    "    save = (save- 127.5) / 127.5\n",
    "    return (isthisEnd, save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "\n",
    "\n",
    "def repeat_trans_conv(model, size):\n",
    "    model.add(Conv2DTranspose(size,(5,5), strides=(2,2), padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "\n",
    "def repeat_conv(model, size):\n",
    "    model.add(Conv2D(size, (5,5), strides=(2,2), padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "\n",
    "def accuracy(mat1, mat2):\n",
    "    mat1 = np.argmax(mat1,axis=1)\n",
    "    mat2 = np.argmax(mat2,axis=1)\n",
    "    diff = mat1-mat2\n",
    "    n_tot = mat1.shape[0]\n",
    "    n_rig = (diff==0).sum()\n",
    "    acc = n_rig*100.0/n_tot\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(input_dim=100, units=4*4*1024))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Reshape((4,4,1024), input_shape=(1024*4*4,)))\n",
    "    repeat_trans_conv(model, 512)\n",
    "    repeat_trans_conv(model, 256)\n",
    "    repeat_trans_conv(model, 128)\n",
    "    #repeat_trans_conv(model, 64)\n",
    "    model.add(Conv2DTranspose(3,(5,5), strides=(2,2), padding='same'))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "def discriminator():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(128, (5,5), strides=(2,2), padding='same', input_shape = (size,size,3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "    #repeat_conv(model, 128)\n",
    "    repeat_conv(model, 256)\n",
    "    repeat_conv(model, 512)\n",
    "    repeat_conv(model, 1024)\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024))\n",
    "    model.add(Dense(2,activation='softmax'))\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "def generator_ppap_dis(g, d):\n",
    "    model = Sequential()\n",
    "    model.add(g)\n",
    "    d.trainable = False\n",
    "    model.add(d)\n",
    "    return model\n",
    "\n",
    "def combine_images(generated_images):\n",
    "    getsoo = int(math.sqrt(generated_images.shape[0]))\n",
    "    output = np.zeros((size * getsoo, size * getsoo, 3), dtype=np.float32)\n",
    "    for i in range(0, getsoo):\n",
    "        for j in range(0, getsoo):\n",
    "            output[size*i:size*i+size, size*j:size*j+size, :] = generated_images[10*i+j]            \n",
    "    output = output * 127.5 + 127.5\n",
    "    output = output.astype(int)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __main__():\n",
    "    g = generator()\n",
    "    d = discriminator()\n",
    "    \n",
    "    d_with_g = generator_ppap_dis(g,d)\n",
    "    d_optim = Adam(lr=0.0002, beta_1=0.5)\n",
    "    g_optim = Adam(lr=0.0002, beta_1=0.5)\n",
    "    \n",
    "    g.compile(loss='binary_crossentropy', optimizer=\"SGD\")\n",
    "    #g.load_weights('generator')\n",
    "    d_with_g.compile(loss='categorical_crossentropy', optimizer=g_optim)\n",
    "    d.trainable=True\n",
    "    d.compile(loss='categorical_crossentropy', optimizer=d_optim)\n",
    "    #d.load_weights('discriminator')\n",
    "    for epoch in range(100):\n",
    "        print(\"Epoch is\", epoch)\n",
    "        print(\"Number of batches\", int(size_of_data/BATCH_SIZE))\n",
    "        flag = False\n",
    "        index = 0\n",
    "        while(not flag):\n",
    "            isthisEnd, image_batch = nextBatch(BATCH_SIZE)\n",
    "            flag = isthisEnd\n",
    "            noise = np.random.uniform(-1, 1, size=(BATCH_SIZE, 100))\n",
    "            generated_images = g.predict(noise)\n",
    "            if index % 150 == 0:\n",
    "                image = combine_images(generated_images)\n",
    "                cv2.imwrite('res'+str(index)+'.png', image)\n",
    "            X = np.concatenate((image_batch, generated_images))\n",
    "            y = np.zeros([2*BATCH_SIZE,2])\n",
    "            y[:BATCH_SIZE,1] = 1\n",
    "            y[BATCH_SIZE:,0] = 1\n",
    "            \n",
    "            d_loss = d.train_on_batch(X, y)\n",
    "            d.trainable = False\n",
    "            y2 = np.zeros([BATCH_SIZE,2])\n",
    "            y2[:,1] = 1\n",
    "            g_loss = d_with_g.train_on_batch(noise, y2)\n",
    "            g_loss = d_with_g.train_on_batch(noise, y2)\n",
    "            d.trainable = True\n",
    "            \n",
    "            noise = np.random.uniform(-1, 1, (BATCH_SIZE, 100))\n",
    "            if index % 10 == 0:\n",
    "                print('d loss: '+str(d_loss)+' g loss: '+str(g_loss))\n",
    "                d_predict = d.predict(X, verbose=0)\n",
    "                with open('dis_res.txt','w') as file:\n",
    "                    file.write(str(d_predict)+'\\n')\n",
    "                d_predict = d_predict >= 0.5\n",
    "                with open('dis_round_int.txt','w') as file:\n",
    "                    file.write(str(np.equal(d_predict, y)))\n",
    "                with open('d_with_g_res.txt', 'w') as file:\n",
    "                    file.write(str(d_with_g.predict(noise, verbose=0)))\n",
    "                d_predict = d_predict.astype(int)\n",
    "                d_acc = accuracy(d_predict, y)\n",
    "                print('d_acc : '+str(d_acc))\n",
    "                with open('disloss.txt', 'a') as file:\n",
    "                    file.write(str(d_loss)+'\\n')\n",
    "                with open('genloss.txt', 'a') as file:\n",
    "                    file.write(str(g_loss)+'\\n')\n",
    "                with open('dis_acc.txt','a') as file:\n",
    "                    file.write(str(d_acc)+'\\n')\n",
    "            if index % 100 == 9:\n",
    "                g.save_weights('generator', True)\n",
    "                d.save_weights('discriminator', True)\n",
    "            index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 16384)             1654784   \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16384)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 16384)             65536     \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 4, 4, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 8, 8, 512)         13107712  \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 8, 8, 512)         2048      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 16, 16, 256)       3277056   \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 16, 16, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 32, 32, 128)       819328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 32, 32, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTr (None, 64, 64, 3)         9603      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 64, 64, 3)         0         \n",
      "=================================================================\n",
      "Total params: 18,937,603\n",
      "Trainable params: 18,903,043\n",
      "Non-trainable params: 34,560\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 128)       9728      \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 32, 32, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 16, 256)       819456    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 16, 16, 256)       1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 8, 8, 512)         3277312   \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 8, 8, 512)         2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 4, 4, 1024)        13108224  \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 4, 4, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 4, 4, 1024)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 16384)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1024)              16778240  \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 2050      \n",
      "=================================================================\n",
      "Total params: 34,002,690\n",
      "Trainable params: 33,998,850\n",
      "Non-trainable params: 3,840\n",
      "_________________________________________________________________\n",
      "Epoch is 0\n",
      "Number of batches 2025\n",
      "d loss: 0.65174 g loss: 1.19209e-07\n",
      "d_acc : 100.0\n",
      "d loss: 1.19209e-07 g loss: 1.19209e-07\n",
      "d_acc : 100.0\n",
      "d loss: 1.19209e-07 g loss: 1.19209e-07\n",
      "d_acc : 79.0\n",
      "d loss: 1.19209e-07 g loss: 1.19209e-07\n",
      "d_acc : 57.0\n",
      "d loss: 1.19209e-07 g loss: 1.19209e-07\n",
      "d_acc : 50.0\n",
      "d loss: 1.19209e-07 g loss: 1.19209e-07\n",
      "d_acc : 50.0\n",
      "d loss: 1.19209e-07 g loss: 1.19209e-07\n",
      "d_acc : 50.0\n",
      "d loss: 1.19209e-07 g loss: 1.19209e-07\n",
      "d_acc : 50.0\n",
      "d loss: 1.19209e-07 g loss: 1.19209e-07\n",
      "d_acc : 50.5\n",
      "d loss: 1.19209e-07 g loss: 1.19209e-07\n",
      "d_acc : 50.0\n",
      "d loss: 1.19209e-07 g loss: 1.19209e-07\n",
      "d_acc : 50.0\n",
      "d loss: 1.19209e-07 g loss: 1.19209e-07\n",
      "d_acc : 50.0\n",
      "d loss: 1.19209e-07 g loss: 1.19209e-07\n",
      "d_acc : 50.0\n",
      "d loss: 1.19209e-07 g loss: 1.19209e-07\n",
      "d_acc : 50.5\n",
      "d loss: 1.19209e-07 g loss: 1.19209e-07\n",
      "d_acc : 50.0\n",
      "d loss: 1.19209e-07 g loss: 1.19209e-07\n",
      "d_acc : 52.0\n",
      "d loss: 1.19209e-07 g loss: 1.19209e-07\n",
      "d_acc : 53.0\n",
      "d loss: 1.19209e-07 g loss: 1.19209e-07\n",
      "d_acc : 54.5\n",
      "d loss: 1.19209e-07 g loss: 1.19209e-07\n",
      "d_acc : 59.5\n",
      "d loss: 1.19209e-07 g loss: 1.19209e-07\n",
      "d_acc : 62.5\n",
      "d loss: 1.19209e-07 g loss: 1.19209e-07\n",
      "d_acc : 69.5\n",
      "d loss: 1.19209e-07 g loss: 1.19209e-07\n",
      "d_acc : 71.5\n",
      "d loss: 1.19209e-07 g loss: 1.19209e-07\n",
      "d_acc : 82.0\n",
      "d loss: 1.19209e-07 g loss: 1.19209e-07\n",
      "d_acc : 80.5\n",
      "d loss: 1.19209e-07 g loss: 1.19209e-07\n",
      "d_acc : 88.5\n",
      "d loss: 1.19209e-07 g loss: 1.19209e-07\n",
      "d_acc : 89.0\n",
      "d loss: 1.19209e-07 g loss: 1.19209e-07\n",
      "d_acc : 93.5\n",
      "d loss: 1.19209e-07 g loss: 1.19209e-07\n",
      "d_acc : 92.5\n",
      "d loss: 1.19209e-07 g loss: 1.19209e-07\n",
      "d_acc : 93.5\n",
      "d loss: 1.19209e-07 g loss: 1.19209e-07\n",
      "d_acc : 97.5\n",
      "d loss: 1.19209e-07 g loss: 1.19209e-07\n",
      "d_acc : 98.5\n",
      "d loss: 1.19209e-07 g loss: 1.19209e-07\n",
      "d_acc : 98.5\n",
      "d loss: 1.19209e-07 g loss: 1.19209e-07\n",
      "d_acc : 98.0\n",
      "d loss: 1.19209e-07 g loss: 1.19209e-07\n",
      "d_acc : 100.0\n",
      "d loss: 1.19209e-07 g loss: 1.19209e-07\n",
      "d_acc : 97.5\n",
      "d loss: 1.19209e-07 g loss: 1.19209e-07\n",
      "d_acc : 100.0\n",
      "d loss: 1.19209e-07 g loss: 1.19209e-07\n",
      "d_acc : 99.5\n",
      "d loss: 1.19209e-07 g loss: 1.19209e-07\n",
      "d_acc : 100.0\n",
      "d loss: 1.19209e-07 g loss: 1.19209e-07\n",
      "d_acc : 100.0\n",
      "d loss: 1.19209e-07 g loss: 1.19209e-07\n",
      "d_acc : 100.0\n",
      "d loss: 1.19209e-07 g loss: 1.19209e-07\n",
      "d_acc : 100.0\n",
      "d loss: 1.19209e-07 g loss: 1.19209e-07\n",
      "d_acc : 99.5\n",
      "d loss: 1.19209e-07 g loss: 1.19209e-07\n",
      "d_acc : 100.0\n",
      "d loss: 1.19209e-07 g loss: 1.19209e-07\n",
      "d_acc : 100.0\n",
      "d loss: 1.19209e-07 g loss: 1.19209e-07\n",
      "d_acc : 100.0\n",
      "d loss: 1.19209e-07 g loss: 1.19209e-07\n",
      "d_acc : 100.0\n",
      "d loss: 1.19209e-07 g loss: 1.19209e-07\n",
      "d_acc : 100.0\n",
      "d loss: 1.19209e-07 g loss: 1.19209e-07\n",
      "d_acc : 100.0\n",
      "d loss: 1.19209e-07 g loss: 1.19209e-07\n",
      "d_acc : 100.0\n",
      "d loss: 1.19209e-07 g loss: 1.19209e-07\n",
      "d_acc : 100.0\n",
      "d loss: 1.19209e-07 g loss: 1.19209e-07\n",
      "d_acc : 100.0\n",
      "d loss: 1.19209e-07 g loss: 1.19209e-07\n",
      "d_acc : 100.0\n",
      "d loss: 1.19209e-07 g loss: 1.19209e-07\n",
      "d_acc : 100.0\n",
      "d loss: 1.19209e-07 g loss: 1.19209e-07\n",
      "d_acc : 100.0\n",
      "d loss: 1.19209e-07 g loss: 1.19209e-07\n",
      "d_acc : 100.0\n",
      "d loss: 1.19209e-07 g loss: 1.19209e-07\n",
      "d_acc : 100.0\n",
      "d loss: 1.19209e-07 g loss: 1.19209e-07\n",
      "d_acc : 100.0\n",
      "d loss: 1.19209e-07 g loss: 1.19209e-07\n",
      "d_acc : 100.0\n",
      "d loss: 1.19209e-07 g loss: 1.19209e-07\n",
      "d_acc : 100.0\n",
      "d loss: 1.19209e-07 g loss: 1.19209e-07\n",
      "d_acc : 100.0\n",
      "d loss: 1.19209e-07 g loss: 1.19209e-07\n",
      "d_acc : 100.0\n",
      "d loss: 1.19209e-07 g loss: 1.19209e-07\n",
      "d_acc : 100.0\n",
      "d loss: 1.19209e-07 g loss: 1.19209e-07\n",
      "d_acc : 100.0\n",
      "d loss: 1.19209e-07 g loss: 1.19209e-07\n",
      "d_acc : 100.0\n",
      "d loss: 1.19209e-07 g loss: 1.19209e-07\n",
      "d_acc : 100.0\n",
      "d loss: 1.19209e-07 g loss: 1.19209e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_acc : 100.0\n",
      "d loss: 1.19209e-07 g loss: 1.19209e-07\n",
      "d_acc : 100.0\n",
      "d loss: 1.19209e-07 g loss: 1.19209e-07\n",
      "d_acc : 100.0\n",
      "d loss: 1.19209e-07 g loss: 1.19209e-07\n",
      "d_acc : 100.0\n",
      "d loss: 1.19209e-07 g loss: 1.19209e-07\n",
      "d_acc : 100.0\n",
      "d loss: 1.19209e-07 g loss: 1.19209e-07\n",
      "d_acc : 100.0\n",
      "d loss: 1.19209e-07 g loss: 1.19209e-07\n",
      "d_acc : 100.0\n",
      "d loss: 1.19209e-07 g loss: 1.19209e-07\n",
      "d_acc : 100.0\n",
      "d loss: 1.19209e-07 g loss: 1.19209e-07\n",
      "d_acc : 100.0\n",
      "d loss: 1.19209e-07 g loss: 1.19209e-07\n",
      "d_acc : 100.0\n",
      "d loss: 1.19209e-07 g loss: 1.19209e-07\n",
      "d_acc : 100.0\n",
      "d loss: 1.19209e-07 g loss: 1.19209e-07\n",
      "d_acc : 100.0\n",
      "d loss: 1.19209e-07 g loss: 1.19209e-07\n",
      "d_acc : 100.0\n",
      "d loss: 1.19209e-07 g loss: 1.19209e-07\n",
      "d_acc : 100.0\n",
      "d loss: 1.19209e-07 g loss: 1.19209e-07\n",
      "d_acc : 100.0\n",
      "d loss: 1.19209e-07 g loss: 1.19209e-07\n",
      "d_acc : 100.0\n",
      "d loss: 1.19209e-07 g loss: 1.19209e-07\n",
      "d_acc : 100.0\n",
      "d loss: 1.19209e-07 g loss: 1.19209e-07\n",
      "d_acc : 100.0\n",
      "d loss: 1.19209e-07 g loss: 1.19209e-07\n",
      "d_acc : 100.0\n",
      "d loss: 1.19209e-07 g loss: 1.19209e-07\n",
      "d_acc : 100.0\n"
     ]
    }
   ],
   "source": [
    "__main__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
